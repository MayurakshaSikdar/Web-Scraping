***
***
# Web Scraping using Python3
***
***
##### _Submitted by_
## Mayuraksha Sikdar
### _B. Tech in Computer Science & Engineering_
---
<br />


### 1. ABSTRACT: 
 Web Scraping is a set of methods, which allows a user to collect information presented on the World Wide Web (WWW). It is a similar technology which is used by search engines marked as Web Crawling. Although web scraping is not a new term, in years past the practice has been more commonly known as screen scraping, data mining, web harvesting, or similar variations.
In this project, web scraping is used to collect data from a website, which is listed as a ‘free to scrape’ website. Other websites are not being scraped due to legal issues.

<br /><br />
### 2. INTRODUCTION: 

In theory, web scraping is the practice of gathering data through any means other than a program interacting with an API (or, obviously, through a human using a web browser). This is most commonly accomplished by writing an automated program that queries a web server, requests data (usually in the form of the HTML and other files that comprise web pages), and then parses that data to extract needed information. 
In practice, web scraping encompasses a wide variety of programming techniques and technologies, such as data analysis and information security. Web scrapers are excellent at gathering and processing large amounts of data (among other things). Rather than viewing one page at a time through the narrow window of a monitor, you can view databases spanning thousands or even millions of pages at once. In addition, web scrapers can go places that traditional search engines cannot. A Google search for “cheapest flights to Boston” will result in a slew of advertisements and popular flight search sites. Google only knows what these websites say on their content pages, not the exact results of various queries entered into a flight search application. However, a well-developed web scraper can chart the cost of a flight to Boston over time, across a variety of websites, and tell you the best time to buy your ticket.

<br /><br />
### 3. WEB SCRAPING:
Web scraping, also known as web extraction or harvesting, is a technique to extract data from the World Wide Web (WWW) and save it to a file system or database for later retrieval or analysis. Commonly, web data is scrapped utilizing Hypertext Transfer Protocol (HTTP) or through a web browser. This is accomplished either manually by a user or automatically by a bot or web crawler. Due to the fact that an enormous amount of heterogeneous data is constantly generated on the WWW, web scraping is widely acknowledged as an efficient and powerful technique for collecting big data.

<br /><br />
### 4. PURPOSE OF WEB SCRAPING:<br />
#### 4.1	Market analysis and research:

Data collection from online sources became one of the market research methods. It offers much faster response, compared to a classical surveying. Consumers are active in the online world and share their experience, frustration or motivation. Targeted data collection from e-shop and advertising servers helps to update Indexes. Which are based on frequently changed prices. With the increasing relevance and availability of on-line prices that we see today, it is natural to ask whether the prediction of the consumer price index (CPI), or related statistics, may usefully be computed more frequently than existing monthly schedules allow for.

<br />
#### 4.2 Enterprise technologies:

Incompatible enterprise technologies are common by larger projects. Still a unified presentation of data from several systems is necessary. In some specific cases the solution is based on Web Scraping.

<br />
#### 4.3 Opinion Poll:

Movie Producers collect data about their current blockbusters. Such data includes the user feedback, if this was shared on the movie portals in a review.

<br />
#### 4.4	Human Resources Agencies:

Human resource (HR) departments in large companies process many jobs offers for their companies and try to match the position with prospective employees. It’s not sufficient to use only incoming vacancy requests from candidates. HR Departments also cooperate with 3rd Party companies, which can offer them own directories of professionals. Contact mining is an important activity for such agencies.

<br />
#### 4.5	Social Network mining:

Social media (such as blogs, online social networks, microblogs) has become one of the major data sources for quantitative communication research over the past decade. By employing simple programing tools, researchers can extract relevant messages from social media platforms for various research purposes.

<br />
#### 4.6	Government Services:

The Monitoring of criminal activities on social websites and specific forums is an important source of information for Government Agencies and Law Enforcement bodies. 

<br />
#### 4.7	Corporate spying:

In the corporate context, web scraping allows for a company to review both their own and the appearance of competitors in headlines of news servers. A company can also collect details about competitors and even about its own employees.

<br />
#### 4.8	Social Mining and Sentiment Analysis:

Social media is a new source of data that is significantly different from conventional ones. Social media data sets are mostly user-generated, and are big, interlinked, and heterogeneous.

<br /><br />
### 5.	METHODS OF WEB SCRAPING:
<br />

#### 5.1 Manual Scraping:

Manual scraping is still an option in specific situations. These situations are:

•	When the amount of data is minimal,
•	When the data being scraped does not require a repetitive task,
•	When setting up automated scraping would take longer than the data collection itself.
•	Possibly security measures or specific characteristics of the website do not allow automated methods.

<br />
#### 5.2	HTML Parsing:

Web sites don’t always provide their data in comfortable formats such as .csv or .json files. HTML Pages are created by the server as a response to a user’s request. At this point server software is not relevant, rather the output in the browser is important.

<br />
#### 5.3	DOM Parsing: 

Document Object Model (DOM) Parsing is an evolution of HTML Parsing based on developments of the language and browsers which lead to the introduction of the Document Object Model. DOM is heavily used for Cascading Stylesheets (CSS) and JavaScript. Integration of DOM revealed new possibilities for addressing some specific parts of the webpage.

<br />
#### 5.4	XPath:

Similar addressing possibility as DOM provides XPath (XML Path Language). The name suggests a usage for XML documents. It is applicable also to HTML format. XPath requires a more precisely structured webpage than DOM and has the same possibility to address segments within the webpage. 

<br />
#### 5.5	APIs:

Application Programming Interface (API) expects an application as a communication partner. A standard HTTP Request sent to an API Endpoint returns an answer from server. Each API has its own specification and options. The format of the answer can be set as option in the request. The most widely used format for API communication is JSON.

<br /><br />
### 6.	TOOLS/PLATFORM, HARDWARE AND SOFTWARE REQUIREMENT SPECIFICATIONS:

<br />
#### 6.1	Tools:

• Microsoft VS Code
• Python 3.7
• MS Office
• Internet Connection
• Beautiful Soup python module

<br />
#### 6.2	Platform

•	Windows 7/8/8.1/10

<br />
#### 6.3	Hardware Requirements:

HDD	1 GB
Processor	Pentium 4 or newer processor
Memory	2 GB

<br />
#### 6.4	Software Requirements:

Python	Version 3.X
Microsoft VS Code	Version 1.X
BeautifulSoup module	Version 4.X
Random module	Version 2.X
Requests module	Version 2.X

<br />
### 7.	GOALS OF IMPLEMENTATION:

To extract data from website using concept and tools of web scraping in python programming language.

<br />
